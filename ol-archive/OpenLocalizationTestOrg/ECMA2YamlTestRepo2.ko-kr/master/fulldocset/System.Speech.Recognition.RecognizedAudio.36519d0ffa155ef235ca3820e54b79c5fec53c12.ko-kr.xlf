<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.2" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="markdown" source-language="en-US" target-language="ko-kr">
    <header>
      <tool tool-id="mdxliff" tool-name="mdxliff" tool-version="1.0-192e1fd" tool-company="Microsoft" />
      <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">71890fe70f6a6f16648164fa74fd58f42e758bb3</xliffext:olfilehash>
      <xliffext:olfilepath xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">fulldocset\System.Speech.Recognition.RecognizedAudio.yml</xliffext:olfilepath>
      <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">fulldocset</xliffext:oltranslationpriority>
      <xliffext:oltranslationtype xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">Human Translation</xliffext:oltranslationtype>
      <xliffext:olskeletonhash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">f6fd3f0a8d298c74a7c3ad5af2b6e3caca05368e</xliffext:olskeletonhash>
      <xliffext:olxliffhash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">f509861ae4e9898852fd1c79862ea5987f709a7c</xliffext:olxliffhash>
    </header>
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Represents audio input that is associated with a <bpt id="p1">&lt;xref href="System.Speech.Recognition.RecognitionResult"&gt;</bpt><ept id="p1">&lt;/xref&gt;</ept>.</source>
          <target state="translated">와 연결 된 나타냅니다 오디오 즉 입력 한 <bpt id="p1">&lt;xref href="System.Speech.Recognition.RecognitionResult"&gt;</bpt> <ept id="p1">&lt;/xref&gt;</ept>합니다.</target>       </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve" extradata="MT">
          <source>A speech recognizer generates information about the audio input as part of the recognition operation.</source>
          <target state="translated">음성 인식기에서 인식 작업의 일부로 오디오 입력에 대 한 정보를 생성합니다.</target>       </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve" extradata="MT">
          <source>To access the recognized audio, use the &lt;xref:System.Speech.Recognition.RecognitionResult.Audio%2A&gt; property or the &lt;xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A&gt; method of the &lt;xref:System.Speech.Recognition.RecognitionResult&gt;.</source>
          <target state="translated">인식 된 오디오를 액세스 하려면 사용 &lt;xref:System.Speech.Recognition.RecognitionResult.Audio%2A&gt;속성 또는 &lt;xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A&gt; &lt;xref:System.Speech.Recognition.RecognitionResult&gt;.&lt;/xref:System.Speech.Recognition.RecognitionResult&gt; 메서드&lt;/xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A&gt; &lt;/xref:System.Speech.Recognition.RecognitionResult.Audio%2A&gt;</target>       </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve" extradata="MT">
          <source>A recognition result can be produced by the following events and methods of the &lt;xref:System.Speech.Recognition.SpeechRecognizer&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt; classes:      -   Events:          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName&gt;          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName&gt;          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName&gt;          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName&gt;          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName&gt;      -   Methods:          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName&gt;          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName&gt;          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName&gt;          -   &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName&gt;      &gt; <ph id="ph1">[!IMPORTANT]</ph> &gt;  A recognition result produced by emulated speech recognition does not contain recognized audio.</source>
          <target state="translated">인식 결과 다음과 같은 이벤트 및의 메서드에 의해 생성 될 수 있습니다는 &lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;및 &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;클래스:-이벤트:- &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName&gt;및 &lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName&gt;- &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName&gt;및 &lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName&gt;- &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName&gt;및 &lt;xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName&gt;- &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName&gt;및 &lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName&gt;- &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName&gt;-방법:- &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName&gt;및 &lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName&gt;- &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName&gt;및 &lt;xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName&gt;- &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName&gt;- &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName&gt;&gt; <ph id="ph1">[!IMPORTANT]</ph> &gt; 에뮬레이트된 음성 인식에서 생성 된 결과는 인식 된 오디오 A 인식.&lt;/xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognitionEngine&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognizer&gt;</target>       </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve" extradata="MT">
          <source>For such a recognition result, its &lt;xref:System.Speech.Recognition.RecognitionResult.Audio%2A&gt; property returns <ph id="ph1">`null`</ph> and its &lt;xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A&gt; method throws an exception.</source>
          <target state="translated">이러한는 인식 결과 대 한 해당 &lt;xref:System.Speech.Recognition.RecognitionResult.Audio%2A&gt;속성에서 반환 <ph id="ph1">`null`</ph> 및 해당 &lt;xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A&gt;메서드에서 예외를 throw 합니다.&lt;/xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A&gt; &lt;/xref:System.Speech.Recognition.RecognitionResult.Audio%2A&gt;</target>       </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve" extradata="MT">
          <source>For more information about emulated speech recognition, see the <ph id="ph1">`EmulateRecognize`</ph> and <ph id="ph2">`EmulateRecognizeAsync`</ph> methods of the &lt;xref:System.Speech.Recognition.SpeechRecognizer&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt; classes.</source>
          <target state="translated">에뮬레이트된 음성 인식 하는 방법에 대 한 자세한 내용은 참조는 <ph id="ph1">`EmulateRecognize`</ph> 및 <ph id="ph2">`EmulateRecognizeAsync`</ph> 의 메서드는 &lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;및 &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;클래스.&lt;/xref:System.Speech.Recognition.SpeechRecognitionEngine&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognizer&gt;</target>       </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Gets the location in the input audio stream for the start of the recognized audio.</source>
          <target state="translated">인식 된 오디오를 시작 하 여 입력 오디오 스트림의 위치를 가져옵니다.</target>       </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve" extradata="MT">
          <source>This property references the position at the beginning of the recognized phrase in the input device's generated audio stream.</source>
          <target state="translated">이 속성은 생성 된 오디오 스트림 입력된 장치에서에서 인식된 된 구와의 시작 부분에 위치를 참조합니다.</target>       </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve" extradata="MT">
          <source>By contrast, the <ph id="ph1">`RecognizerAudioPosition`</ph> property of the &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt; and &lt;xref:System.Speech.Recognition.SpeechRecognizer&gt; classes reference the recognizer's position within its audio input.</source>
          <target state="translated">반면,는 <ph id="ph1">`RecognizerAudioPosition`</ph> 속성은 &lt;xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;및 &lt;xref:System.Speech.Recognition.SpeechRecognizer&gt;오디오 입력 내의 인식기에서 위치를 참조 하는 클래스입니다.&lt;/xref:System.Speech.Recognition.SpeechRecognizer&gt; &lt;/xref:System.Speech.Recognition.SpeechRecognitionEngine&gt;</target>       </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve" extradata="MT">
          <source>These positions can be different.</source>
          <target state="translated">이 위치는 다를 수 있습니다.</target>       </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve" extradata="MT">
          <source>For more information, see <bpt id="p1">[</bpt>Using Speech Recognition Events<ept id="p1">](http://msdn.microsoft.com/en-us/01c598ca-2e0e-4e89-b303-cd1cef9e8482)</ept>.</source>
          <target state="translated">자세한 내용은 참조 <bpt id="p1">[</bpt>음성 인식 이벤트를 사용 하 여<ept id="p1">](http://msdn.microsoft.com/en-us/01c598ca-2e0e-4e89-b303-cd1cef9e8482)</ept>합니다.</target>       </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve" extradata="MT">
          <source>The &lt;xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A&gt; property gets the system time at the start of the recognition operation.</source>
          <target state="translated">&lt;xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A&gt;속성 인식 작업의 시작 부분에 시스템 시간을 가져옵니다.&lt;/xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A&gt;</target>       </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>The location in the input audio stream for the start of the recognized audio.</source>
          <target state="translated">인식 된 오디오의 시작 부분에 대 한 입력된 오디오 스트림 위치입니다.</target>       </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Gets the duration of the input audio stream for the recognized audio.</source>
          <target state="translated">인식 된 오디오 입력된 오디오 스트림의 기간을 가져옵니다.</target>       </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>The duration within the input audio stream for the recognized audio.</source>
          <target state="translated">인식 된 오디오 입력된 오디오 스트림 내 기간입니다.</target>       </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Gets the format of the audio processed by a recognition engine.</source>
          <target state="translated">인식 엔진에서 처리 하는 오디오의 형식을 가져옵니다.</target>       </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>The format of the audio processed by the speech recognizer.</source>
          <target state="translated">음성 인식기에서 처리 하는 오디오의 형식입니다.</target>       </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Selects and returns a section of the current recognized audio as binary data.</source>
          <target state="translated">선택 하 고 이진 데이터로 오디오 인식 현재의 일정 부분을 반환 합니다.</target>       </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>The starting point of the audio data to be returned.</source>
          <target state="translated">반환 되는 오디오 데이터의 시작 지점입니다.</target>       </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>The length of the segment to be returned.</source>
          <target state="translated">반환 되는 세그먼트의 길이입니다.</target>       </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Returns a subsection of the recognized audio, as defined by <bpt id="p1">&lt;code&gt;</bpt><ph id="ph1">audioPosition</ph><ept id="p1">&lt;/code&gt;</ept> and <bpt id="p2">&lt;code&gt;</bpt><ph id="ph2">duration</ph><ept id="p2">&lt;/code&gt;</ept>.</source>
          <target state="translated">에 정의 된 대로 인식 된 오디오의 하위 섹션 반환 <bpt id="p1">&lt;code&gt;</bpt> <ph id="ph1">audioPosition</ph> <ept id="p1">&lt;/code&gt;</ept> 및 <bpt id="p2">&lt;code&gt;</bpt> <ph id="ph2">duration</ph> <ept id="p2">&lt;/code&gt;</ept>합니다.</target>       </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;code&gt;audioPosition&lt;/code&gt;</ph> and <ph id="ph2">&lt;code&gt;duration&lt;/code&gt;</ph> define a segment of audio outside the range of the current segment.</source>
          <target state="translated"><ph id="ph1">&lt;code&gt;audioPosition&lt;/code&gt;</ph>및 <ph id="ph2">&lt;code&gt;duration&lt;/code&gt;</ph> 오디오 현재 세그먼트의 범위 밖에 있는 세그먼트를 정의 합니다.</target>       </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>The current recognized audio contains no data.</source>
          <target state="translated">현재 인식 오디오에 데이터가 없습니다.</target>       </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Gets the system time at the start of the recognition operation.</source>
          <target state="translated">인식 작업의 시작 부분에 시스템 시간을 가져옵니다.</target>       </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve" extradata="MT">
          <source>The StartTime property gets the system time at the start of the recognition operation, which can be useful for latency and performance calculations.</source>
          <target state="translated">StartTime 속성 대기 시간 및 성능 계산에 유용할 수 있는 인식 작업의 시작 부분에 시스템 시간을 가져옵니다.</target>       </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve" extradata="MT">
          <source>The &lt;xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A&gt; property gets the location in the input device's generated audio stream.</source>
          <target state="translated">&lt;xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A&gt;속성이 생성 된 오디오 스트림 입력된 장치에서에서 위치를 가져옵니다.&lt;/xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A&gt;</target>       </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>The system time at the start of the recognition operation.</source>
          <target state="translated">인식 작업의 시작 부분에 시스템 시간입니다.</target>       </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>Writes the entire audio to a stream as raw data.</source>
          <target state="translated">전체 오디오 원시 데이터를 스트림에 씁니다.</target>       </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve" extradata="MT">
          <source>Audio data is written to <ph id="ph1">`outputStream`</ph> in binary form.</source>
          <target state="translated">오디오 데이터에 기록 됩니다 <ph id="ph1">`outputStream`</ph> 이진 형식에서입니다.</target>       </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve" extradata="MT">
          <source>No header information is included.</source>
          <target state="translated">없음 헤더 정보가 포함 됩니다.</target>       </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve" extradata="MT">
          <source>The WriteToAudioStream method uses the Wave format, but does not include the Wave header.</source>
          <target state="translated">WriteToAudioStream 메서드 웨이브 형식을 사용 하지만 웨이브 헤더를 포함 하지 않습니다.</target>       </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve" extradata="MT">
          <source>To include the Wave header, use the &lt;xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A&gt; method.</source>
          <target state="translated">사용 하 여 웨이브 헤더를 포함 하려면는 &lt;xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A&gt;메서드.&lt;/xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A&gt;</target>       </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>The stream that will receive the audio data.</source>
          <target state="translated">오디오 데이터를 받을 스트림.</target>       </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>Writes audio to a stream in Wave format.</source>
          <target state="translated">오디오 웨이브 형태로 표시 스트림에 씁니다.</target>       </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve" extradata="MT">
          <source>Audio data is written to <ph id="ph1">`outputStream`</ph> in Wave format, which includes a resource interchange file format (RIFF) header.</source>
          <target state="translated">오디오 데이터에 기록 됩니다 <ph id="ph1">`outputStream`</ph> 리소스 교환 파일 형식 (RIFF) 헤더에서 웨이브 형식을 포함 합니다.</target>       </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve" extradata="MT">
          <source>The &lt;xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A&gt; method uses the same binary format, but does not include the Wave header.</source>
          <target state="translated">&lt;xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A&gt;메서드 같은 이진 형식을 사용 하지만 웨이브 헤더를 포함 하지 않습니다.&lt;/xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A&gt;</target>       </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>The stream that will receive the audio data.</source>
          <target state="translated">오디오 데이터를 받을 스트림.</target>       </trans-unit>
      </group>
    </body>
  </file>
</xliff>